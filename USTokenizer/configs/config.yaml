
model:
  llama_path: "path/to/llama_model" # Llama model path
  end_sym: "<|end_of_text|>" 
  whisper_path: "path/to/whisper_path/whisper-medium" # whisper model path
  ckpt: "" # if not "", load model from ckpt for training or evaluation
  freeze_whisper: True


  # LoRA
  lora: False
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.1

  multi_prompt: True
  prompt_template: "USER: {}\nASSISTANT:"
  prompt_path: "prompts/train_prompt.json"
  test_prompt_path: "prompts/test_prompt.json"
  max_txt_len: 300

  codec_config:
    # RepCodec config
    input_channels: 1024 # for the d_model size of whisper medium
    output_channels: 1024
    encode_channels: 1024
    decode_channels: 1024
    code_dim: 1024
    codebook_num: 1
    codebook_size: 1024
    bias: true
    enc_ratios: [ 1, 1 ]
    dec_ratios: [ 1, 1 ]
    enc_strides: [ 1, 1 ]  # no downsampling
    dec_strides: [ 1, 1 ]
    enc_kernel_size: 3
    dec_kernel_size: 3
    enc_block_dilations: [ 1, 1 ]
    enc_block_kernel_size: 3
    dec_block_dilations: [ 1, 1 ]
    dec_block_kernel_size: 3

datasets:
  train_ann_path: 
    - path/to/data/train/meta.json #data files to train tokenizer
  valid_ann_path: 
    - path/to/data/dev/meta.json
  test_ann_path: 
    - path/to/data/test/meta.json

  whisper_path: "path/to/whisper_path/whisper-medium" # whisper model path

run:
  # log & settings
  seed: 42
  output_dir: "exp/tokenizer_ckpt" # output directory for model checkpoints
  evaluate: False # if True, only evaluate model on test data
  log_freq: 50
  epoch_based: False
  iters_per_epoch: 10000
  accum_grad_iters: 2
  batch_size_train: 4
  batch_size_eval: 4
  checkpoint_interval: 1
  keep_last_checkpoints: 6
  num_workers: 8
  device: "cuda"
  use_distributed: True
  amp: True
  world_size: 1
  dist_url: "env://"
  
  # optimizer & scheduler
  optims:
    max_epoch: 500
    warmup_steps: 3000
    warmup_start_lr: 1e-6
    init_lr: 3e-5
    min_lr: 1e-5
    weight_decay: 0.05
    beta2: 0.999